---
title: "Random effects in the Bradley-Terry model"
bibliography: bibliography.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

In this vignette, we will go over an example of analyzing optimization algorithms based on benchmark functions. A extended analysis of this example including the experimental conditions for the data collection is discussed in (TODO: ADD REF).

In the development of optimization algorithms it is common to use benchmark functions/problems to show:

* The algorithm works on a number of the artificial problems 
* Compare how different algorithms perform in the same set of benchmark problems that have a known solution.

In the comparison part, researchers usually select a small number of benchmark problems (something like 20-30) and run their algorithms and the algorithms they are comparing several times for the same benchmark (something like 10 times each algorithm in each benchmark). At the end, they compare how close the algorithms approached the solution and rank them. 

Almost always, this comparison is done with non-parametric tests statistics and often fail to compute the intra-class correlation of the repeated-measures, or average the repeated measures (throwing out the uncertainty of the results) before running the tests.

## Why use ranks and not the output directly?

Each benchmark function has a different search space and the algorithms converge quite differently from one function to the other. While the difference between the minimum and the algorithm output is limited to zero (when they solve the problem), the difference can range from 0.1 to 100 depending on the benchmark problem. To use this value would require either transformations such as normalizing for each benchmark. However, if we want only to rank the algorithms and provide a classification between them, the Bradley-Terry model is ideal. The differences in range of each benchmark function is already taken care of when computing who wins and who looses.

In this example, we will show how to use the Bradley-Terry model to compensate the dependent data of the repeated measures and rank the algorithms. Our Bradley-Terry ranking of the abilities also shows the uncertainty of the ranks and the probabilities of one algorithm beating the other in any of the benchmarks.


## The data

First let's load some packages
```{r setup, warning=F, message=F}
library(bpcs)
library(dplyr)
library(tidyr)
library(ggplot2)
library(kableExtra)
library(knitr)
library(gtools)
library(rstan)
#Stan should use parallel cores
# options(mc.cores = parallel::detectCores()) #use multiple cores
options(mc.cores = 2) #CRAN works only with max 2 cores
rstan::rstan_options(auto_write = TRUE)
```


The collected data is available as:
```{r}
data("optimization_algorithms")
knitr::kable(dplyr::sample_n(optimization_algorithms, size = 20),
             caption = "Sample of the data set")
```

In this dataset, we have one main metrics that we can use to compare the algorithms. The `TrueRewardDifference` is the difference between the minimum found by the algorithm and the global minimum of the benchmark function. `simNumber` indicates the number of the replication of the same algorithm in the same benchmark function (but with different seeds). All algorithms had a budget of 10,000 function evaluations per number of dimensions for each benchmark function.

Here we have the following benchmark functions:
```{r}
bm <- unique(optimization_algorithms$Benchmark)
nbm <- length(bm)
knitr::kable(matrix(bm, ncol = 4))
```
To reduce a bit the model sampling time here lets reduce the number of benchmark functions and only use 10 of them.

```{r}
set.seed(1)
bm_reduced <- sample(size = 10, x = bm, replace = F)
optimization_algorithms <- optimization_algorithms %>%
  dplyr::filter(Benchmark %in% bm_reduced)
knitr::kable(matrix(bm_reduced, ncol = 2))
```


Here we have the optimization algorithms:
```{r}
alg <- unique(optimization_algorithms$Algorithm)
nalg <- length(alg)
knitr::kable(alg)
```

# Preparing the data

To analyze this data we first need to convert it to a paired comparison. We will do it in the following way:

1. First we group and rank the algorithms for each benchmark function in each simNumber. If there are ties (not very likely in this case) we will solve them randomly
2. We expand this dataset into a wide format in the ranks. Each new column will contain the rank of the algorithms
3. We expand this wide format to a long format with each of the paired comparisons.

To compute all the paired combinations between the algorithms we will use the `gtools::comb` function
```{r}
comb <- gtools::combinations(n=nalg, r=2, v=seq(1:nalg), repeats.allowed = F)
```

Creating the dataset
```{r}
#1 Grouping and ranking
df_1 <-
  optimization_algorithms %>%  dplyr::group_by(Benchmark, simNumber) %>%
  dplyr::mutate(rankReward = rank(TrueRewardDifference, ties.method = 'random')) %>%
  dplyr::ungroup() %>%
  dplyr::select(-TrueRewardDifference)

#2 Expanding in wide format
df_2 <- df_1 %>%
  tidyr::pivot_wider(names_from = Algorithm,
                     values_from = rankReward) %>%
  dplyr::select(-MaxFevalPerDimensions,-Ndimensions) #dropping some variables that we will not use

#3 Computing the paired comparisons
## Maybe not the most efficient code, but for this number of variables it is fast enough, only a few seconds...
#We first go row by row of the wide data frame df_2 and expand each row into the number of combinations
#available of the different ranks
df_3 <-
  dplyr::tribble( ~ algo0_name, ~ algo1_name, ~ y, ~ simNumber, ~ Benchmark)

for (i in 1:nrow(df_2))
{
  current_row <- df_2[i, ]
  for (j in 1:nrow(comb)) {
    comb_row <- comb[j, ]
    
    algo0_name <- alg[comb_row[1]]
    algo0 <- comb_row[1]
    algo0_rank <- current_row[[1, algo0_name]]
    
    algo1_name <- alg[comb_row[2]]
    algo1 <- comb_row[2]
    algo1_rank <- current_row[[1, algo1_name]]
    
    diff_rank <- algo1_rank - algo0_rank
    y <-
      ifelse(diff_rank < 0, 1, 0) # if rank0 is higher than rank1, algo1 wins
    df_3 <- tibble::add_row(
      df_3,
      algo0_name = algo0_name,
      algo1_name = algo1_name,
      y = y,
      simNumber = current_row$simNumber,
      Benchmark = current_row$Benchmark
    )
  }
}
df_3 <- as.data.frame(df_3)
```

The table below shows how the dataset looks like now.
```{r}
dplyr::sample_n(df_3,size = 20)
```

# The model

To compensate for the repeated measures model we will use the random effects model. The model can be written as:

For the benchmark $k$ we have:
$$P[i \text{ beats } j  | U_k]= \dfrac{exp(\lambda_i + U_{i,k})}{exp(\lambda_i + U_{i,k}) + exp(\lambda_j + U_{j,k})}$$

$$\lambda \sim \text{Normal}(0,3.0)$$
$$U \sim \text{Normal}(0,3.0)$$

Now we can estimate and control the effect of each benchmark in each algorithm when we estimate the ability parameter. This model will estimate the parameters $\lambda$ and $U$ of the model. Note that the model with many parameters and data points might take a few minutes to sample.

Lets also specify a wide prior for the standard deviation of the random effects. This will help the convergence

```{r}
m <-
  bpc(
    data = df_3,
    player0 = 'algo0_name',
    player1 = 'algo1_name',
    result_column = 'y',
    cluster = 'Benchmark',
    model_type = 'bt-U',
    priors = list(prior_U_std = 10.0)
  )
```

## Diagnostics

First let's look at the MCMC convergence with the traceplots and look at the Rhat and neff
```{r}
stanfit<-get_stanfit(m)
t <- as.data.frame(summary(stanfit)$summary)
t <- t[1:7,c(1, 9, 10)] # getting only the first seven parameters the mean Rhat and neff

knitr::kable(t)
```


All seems ok! Let's look now at the traceplots

Traceplots of the chains with the bayesplot library

```{r}
library(bayesplot)
posterior <- rstan::extract(stanfit, inc_warmup = T, permuted = F)
bayesplot::mcmc_trace(
  posterior,
  pars = c(
    "lambda[1]",
    "lambda[2]",
    "lambda[3]",
    "lambda[4]",
    "lambda[5]",
    "lambda[6]",
    "U_std"
  ),
  n_warmup = 1000
)
```

Everything seems in order for us to look at the predictive posterior bar plot. 

```{r}
yrep<-predict(m,df_3,n=100,return_matrix = T)
yrep<-yrep[,1:nrow(df_3)]#to get only the y_pred and not the ties_pred that is always 0
```


```{r}
bayesplot::ppc_bars(y = df_3$y, yrep = yrep) +
  labs(title = 'Bar plot with medians and uncertainty\n intervals superimposed')
```

This sounds ok. So let's investigate the actual parameters and the probabilities of the models

## Summary parameters

In the console, we can use the summary function. It gives the parameters estimates with HPDI, the probability table for every condition of players and benchmarks and we also have a rank estimate.
```{r}
summary(m)
```

Of course we can get these estimates in nicer tables with kable that allows a greater deal of customization

```{r}
knitr::kable(get_hpdi_parameters(m),
      caption = "Parameter distribution with the HPD intervals",
      digits = 3)
```

```{r}
knitr::kable(get_probabilities(m)$Table,
      caption = "Probabilities of one an algorithm beating the other in different benchmarks",
      digits = 3)
```

```{r}
knitr::kable(dplyr::select(get_rank_of_players(m),-PosteriorRank),
      caption = "Ranking the algorithms",
      digits = 2)
```


Plot the HPD intervals
```{r}
hpdi <- get_hpdi_parameters(m) %>%
  dplyr::filter(startsWith(Parameter, "lambda"))

ggplot2::ggplot(hpdi, aes(x = Parameter)) +
  ggplot2::geom_pointrange(aes(ymin = HPD_lower,
                               ymax = HPD_higher,
                               y = Mean)) +
  ggplot2::labs(y = "Estimate", x = "Algorithm", title = "HPDI interval of the strength of the algorithms") +
  ggplot2::coord_flip()

```


We can see that the Differential Evolution Performs better than the others. While the PSO and the CMA-ES algorithm have relatively similar abilities.


From the high value of the $U_{std}$ we can see that the choice of benchmarks greatly have a large variance and a big influence in the results and that some algorithms have a better performance in some benchmarks than others. E.g. the Trefethen function is a difficult problem to solve and the U estimate there seems to reduce the ability of the best algorithms while increasing the ability of the worst ones, acting as a regularizing parameter there.
