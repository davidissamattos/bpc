---
title: "Getting Started with the bpcs package"
bibliography: ./bibliography.bib
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting Started with the bpcs package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, echo=T, results='hide', warning=F, message=F}
library(bpcs)
library(tidyverse)
library(kableExtra)
library(bayesplot)
```

# Getting started
In this vignette, we provide an example of the usage of the `bpcs` package along with the core concepts to use the package.

## Installation
The package requires installation of the `rstan` package [@rstan]. For more details see the `REAMDE.md` document.

To install the latest version from Github:
```{r eval=FALSE, echo=T}
remotes::install_github('davidissamattos/bpcs')
```

After install we load the package with:
```{r}
library(bpcs)
```

## Introduction

The `bpcs` package performs Bayesian estimation of Paired Comparison models utilizing Stan. 
We provide a series of models and auxiliary functions to help in the analysis and evaluation of the models. However, this package have the philosophy of 'batteries not included' for plots, tables and data transformation. There are already many great packages capable of performing create high quality plots, tables and that provides tools for data transformation. Since each user can have their own preferences, customization needs and data cleaning and transformation workflows, we designed not to enforce any particular framework or package. Our functions were designed to return cleaned data frames that can be used almost directly, or with few transformations in those packages. 

With that said, we provide in the vignettes the code we use to transform the data and generate the tables and plots. The user is free to use/copy/modify these codes for their own use. For those we rely on the collection of packages `tidyverse` [@tidyverse2019], and the packages `knitr` [@knitr2014] and `kableExtra` [@kableExtra2020].


# The Bradley Terry model

# Tennis example

In this example, we will use the example from tennis players from Agresti [@agresti2003categorical]. The data `tennis_agresti` contains the information regarding tennis matches between 5 players, and how many times they won against each other.

```{r}
knitr::kable(tennis_agresti) %>% 
  kableExtra::kable_styling()
```

We can expand this data into a series of matches with results of who won and who lost. We can visualize it in the `tennis_agresti_2`.

Although, functions can be created to expand `tennis_agresti` in `tennis_agresti_2` due to the size of this dataset we performed this manually.

```{r}
knitr::kable(tennis_agresti_2) %>% 
  kableExtra::kable_styling()
```

We can fit a Bayesian Bradley-Terry model using the `bpc` function

```{r}
m1 <- bpc(data = tennis_agresti_2,
          player0 = 'player0',
          player1 = 'player1',
          result_column = 'y',
          model_type = 'bradleyterry',
          solve_ties = 'none', #there are no ties
          show_chain_messages = T)
```

## Diagnostics
After the chain converges to find the result we can investigate if everything went right. 
For that we can use the excellent tool provided in the `shinystan` [@shinystan2018] package that helps to assess the convergence of the chains.

The `bpcs` package provides a tiny wrapper to launch it automatically with some default parameters.

```{r eval=F}
launch_shinystan(m1)
```


Alternatively, you can retrieve the stanfit object and launch it with your own parameters.

```{r eval=F}
stanfit <- get_stanfit(m1)
shinystan::launch_shinystan(stanfit)
```

If you prefer to investigate without `shinystan` we can retrieve the stanfit object and investigate ourselves or with the help of the `bayesplot` package [@bayesplot2019]

```{r}
stanfit <- get_stanfit(m1)
posterior<-rstan::extract(stanfit,inc_warmup=T,permuted=F)
```

Getting the traceplots:

```{r}
mcmc_trace(posterior,pars = c("lambda[1]","lambda[2]","lambda[3]","lambda[4]"), n_warmup=1000)
```
From the graph above we see a good mix between the chains.

We can also look at divergent iterations between two parameters
```{r}
mcmc_scatter(
  as.matrix(stanfit),
  pars = c("lambda[1]","lambda[2]"),
  np = nuts_params(stanfit),
  np_style = scatter_style_np(div_color = "green", div_alpha = 0.8)
)
```
In this graph we don't see any between lambda[1] and lambda[2]

Verifying the Rhat and neff using the functions from `rstan`
```{r}
rstan::summary(stanfit ,pars=c('lambda'))$summary
```


## Predictive posterior

We first get the observed values and then the predictive values of the original dataframe. We can get predictive values with the predictive function and passing a data frame with the values we want to predict (in this case the original one). Note that we need to have the same column names in this new data frame

```{r}
y<-as.vector(tennis_agresti_2$y)
yrep<-predict(m1,tennis_agresti_2,n=100,return_matrix = T)
```

Now we plot the predictive posteriors
```{r}
ppc_stat(y=y, yrep=yrep, stat="median")
```

```{r}
ppc_bars(y=y, yrep=yrep) +
  labs(title = 'Bar plot with medians and uncertainty\n intervals superimposed')
```

The  plots indicate a good model as the predictive posterior and the observed values agree largely.

## Parameter tables and plots

Now that we are confident on our model we can create some tables to report our results. 

To see the results in the console the `summary` function provides a good overview of the model.
```{r}
summary(m1)
```

If we want to create nicer tables and export them to latex/html we can leverage this with the `kable` function and the `kableExtra` package. Note that for extensive customization (and examples) we refer to the packages documentation.

Parameter table with HPD intervals
```{r}
kable(get_hpdi_parameters(m1), caption = 'Parameter distribution and the High Posterior Density intervals', digits = 2) %>% 
  kable_styling()
```

A posterior distribution figure. The initial code is to replace the lambda[1] to lambda[4] with the name of the players.
```{r}
#changing the names of the parameters in the posterior 
dimnames<-attr(posterior, 'dimnames')
parameters<-dimnames$parameters #reading the parameters of the posterior
names <- get_par_names(m1,par='lambda')#from bpcs
indexes <- seq(1:length(names))
new_pars <- replace(parameters,indexes,names)
#changin the attributes
dimnames$parameters<-new_pars
attr(posterior,'dimnames') <- dimnames

mcmc_areas(posterior,
           pars = names,
           prob = 0.8)+
  labs(title = 'Posterior distribution with medians\n and 80% intervals')
```

```{r}
kable(get_probabilities(m1), caption = 'Probabilities of one player beating the other', digits = 2) %>% 
  kable_styling() %>% 
  add_header_above(c("P[i beats j]"=2, "Estimates"=3))
```
We might also be interested in ranking the players based on their ability $lambda$. In the Bayesian case, we sample the posterior distribution of $lambda$ and rank them so we have posterior distribution of the ranks. This can be achieve with the function `rank_parameters`.

```{r}
ranking <- rank_parameters(m1)
```

We can produce a table with the values of this dataframe.

```{r}
t <- ranking %>% dplyr::select(Parameter, MedianRank, StdRank)
kable(t, caption = 'Rank of the players') %>%
  kable_styling()
```

If we want to visualize the histogram of the rank distribution of each player. 
```{r}
ggplot()+
  geom_histogram(aes(x=ranking$PosteriorRank[1]$rank),bins = 5)+
  labs(title = 'Posterior distribution of the rank for Graf', x='Rank')
```

## Predicting results

To predict new results we need a data frame similar to the one used to fit the data. We use the same function as in the predicted posterior but now we provide the data we want to predict instead of the original data. Lets predict who is the winner for all games from Seles. Now we don't want to return the matrix but a data frame

```{r}
tennis_new_games<- tibble::tribble(~player0, ~player1,
                                  'Seles', 'Graf',
                                  'Seles', 'Sabatini',
                                  'Seles', 'Navratilova',
                                  'Seles', 'Sanchez')
y_seles<-predict(m1,tennis_new_games,n=100)
#Now let's summarize the posterior
y_seles <- dplyr::mutate(y_seles, avg_win_player1 = rowMeans(select(y_seles, starts_with('y_pred')))) 
y_seles %>% 
  dplyr::select(player0, player1,avg_win_player1) %>%
  kable()
```
If the average number of wins of player 1 is higher than 0.5 than player 1 wins more times than player 0.

Note that this is consistent with the obtained ranking and the probabilities of beating.

## Comparing with aggregated data

We can compare this result with the aggregated data of wins, and consider the number of wins as a score.
Since in the aggregated we are throwing out information on how many times one player beats the other we recommend using the expanded version

```{r}
m2 <- bpc(data = tennis_agresti,
          player0 = 'player0',
          player1 = 'player1',
          player0_score = 'wins_player0', 
          player1_score = 'wins_player1',
          model_type = 'bradleyterry',
          solve_ties = 'random', #now there is one tie and we need to handle it
          show_chain_messages = F)
```

Now let's compare the summary tables of both

Expanded:
```{r}
summary(m1)
```

Reduced:
```{r}
summary(m2)
```

We can see that we have very different probabilities of winning (comparing the strength parameter itself is not recommended)

Comparing the produced ranks we see that we even have a difference in the location fo the ranks for Navratilova and Seles
```{r}
ranking2 <- rank_parameters(m2)
t2 <- ranking2 %>% dplyr::select(Parameter, MedianRank, StdRank)
kable(t, caption = 'Rank of the players Expanded Dataset') %>%
  kable_styling()
kable(t2, caption = 'Rank of the players Aggregated Dataset') %>%
  kable_styling()
```

